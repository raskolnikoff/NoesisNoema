[
  {
    "metadataOutputVersion" : "3.0",
    "shortDescription" : "meta-llama\/Meta-Llama-3-8B-Instruct (feature-extraction)",
    "outputSchema" : [
      {
        "hasShapeFlexibility" : "0",
        "isOptional" : "0",
        "dataType" : "Float32",
        "formattedType" : "MultiArray (Float32 1 × 128 × 4096)",
        "shortDescription" : "Sequence of hidden-states at the output of the last layer of the model",
        "shape" : "[1, 128, 4096]",
        "name" : "last_hidden_state",
        "type" : "MultiArray"
      }
    ],
    "storagePrecision" : "Float16",
    "modelParameters" : [

    ],
    "specificationVersion" : 6,
    "mlProgramOperationTypeHistogram" : {
      "Transpose" : 128,
      "Gather" : 1,
      "ReduceMean" : 65,
      "Silu" : 32,
      "Reshape" : 194,
      "Matmul" : 64,
      "Cast" : 2,
      "Softmax" : 32,
      "Select" : 1,
      "Concat" : 64,
      "Add" : 226,
      "Tile" : 64,
      "Pow" : 65,
      "Rsqrt" : 65,
      "Linear" : 224,
      "Scatter" : 1,
      "ExpandDims" : 66,
      "Equal" : 1,
      "SliceByIndex" : 128,
      "Mul" : 386
    },
    "computePrecision" : "Mixed (Float16, Float32, Int32)",
    "isUpdatable" : "0",
    "stateSchema" : [

    ],
    "availability" : {
      "macOS" : "12.0",
      "tvOS" : "15.0",
      "visionOS" : "1.0",
      "watchOS" : "8.0",
      "iOS" : "15.0",
      "macCatalyst" : "15.0"
    },
    "modelType" : {
      "name" : "MLModelType_mlProgram"
    },
    "inputSchema" : [
      {
        "hasShapeFlexibility" : "0",
        "isOptional" : "0",
        "dataType" : "Int32",
        "formattedType" : "MultiArray (Int32 1 × 128)",
        "shortDescription" : "Indices of input sequence tokens in the vocabulary",
        "shape" : "[1, 128]",
        "name" : "input_ids",
        "type" : "MultiArray"
      },
      {
        "hasShapeFlexibility" : "0",
        "isOptional" : "0",
        "dataType" : "Int32",
        "formattedType" : "MultiArray (Int32 1 × 128)",
        "shortDescription" : "Mask to avoid performing attention on padding token indices (1 = not masked, 0 = masked)",
        "shape" : "[1, 128]",
        "name" : "attention_mask",
        "type" : "MultiArray"
      }
    ],
    "userDefinedMetadata" : {
      "com.github.apple.coremltools.source" : "torch==2.7.1",
      "transformers_version" : "4.40.0.dev0",
      "com.github.apple.coremltools.version" : "8.3.0",
      "com.github.apple.coremltools.source_dialect" : "TorchScript",
      "co.huggingface.exporters.architecture" : "LlamaForCausalLM",
      "co.huggingface.exporters.name" : "meta-llama\/Meta-Llama-3-8B-Instruct",
      "co.huggingface.exporters.framework" : "pytorch",
      "co.huggingface.exporters.task" : "feature-extraction",
      "co.huggingface.exporters.precision" : "float16"
    },
    "generatedClassName" : "Model",
    "method" : "predict"
  }
]