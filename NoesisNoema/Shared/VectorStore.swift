//// VectorStore.swift
/// //  NoesisNoema
/// //  Created by Раскольников on 2025/07/18.


import Foundation

class VectorStore {
    
    /**
        A structure representing a chunk of text with its embedding.
        - `text`: The text content of the chunk.
        - `embedding`: The embedding vector for the text.
        - `metadata`: Optional metadata associated with the chunk.
     */
    var chunks: [Chunk]
    var embeddingModel: EmbeddingModel
    var isEmbedded: Bool
    /// Initializes a VectorStore with an embedding model and an optional initial set of chunks.
    /// - Parameters:
    ///  - embeddingModel: The model used to generate embeddings for text.
    init(embeddingModel: EmbeddingModel, chunks: [Chunk] = []) {
        self.embeddingModel = embeddingModel
        self.chunks = chunks
        self.isEmbedded = false
    }
        
    /**
        クエリ埋め込みベクトルに最も類似したChunkを返す
        - Parameter queryEmbedding: クエリの埋め込みベクトル（[Float]型）
        - Returns: 類似度上位のChunk配列
     */
    func findRelevant(queryEmbedding: [Float], topK: Int = 3) -> [Chunk] {
        // Chunkにembeddingがないため、全件返す or 空配列返す
        // 必要ならcontentの類似度で拡張可能
        return Array(chunks.prefix(topK))
    }
    
    /// Retrieves the top-K most relevant chunks for a given query string using the embedding model.
    /// - Parameters:
    ///   - query: The query string to search for relevant chunks.
    ///   - topK: The number of top relevant chunks to retrieve (default: 3).
    /// - Returns: An array of the most relevant chunks.
    public func retrieveChunks(for query: String, topK: Int = 3) -> [Chunk] {
        // Generate query embedding using the embedding model
        let queryEmbedding = embeddingModel.embed(text: query)
        // Retrieve top-K relevant chunks
        let relevantChunks = findRelevant(queryEmbedding: queryEmbedding, topK: topK)
        return relevantChunks
    }
    
    /// Generates a RAG (Retrieval-Augmented Generation) answer for the given query.
    /// - Parameters:
    ///   - query: The question/query string.
    ///   - topK: The number of top relevant chunks to retrieve (default: 3).
    /// - Returns: The answer string generated by the LLM (placeholder implementation).
    public func ragAnswer(for query: String, topK: Int = 3) -> String {
        // Retrieve relevant chunks for the query
        let relevantChunks = retrieveChunks(for: query, topK: topK)
        // Concatenate their contents to form the context string
        let context = relevantChunks.map { $0.content }.joined(separator: "\n")
        // Construct the prompt
        let prompt = "Context:\n\(context)\n\nQuestion: \(query)\nAnswer:"
        // Call the LLM model to generate an answer (to be implemented in actual integration)
        // Example: let answer = LLMModel.generate(prompt: prompt)
        // TODO: Integrate with LLM for actual answer generation.
        return "RAG answer (to be implemented)"
    }
    
    /// VectorStoreのシングルトン（RAG検索対象チャンクを保持）
    static let shared = VectorStore(embeddingModel: EmbeddingModel(name: "default-embedding"))
}
