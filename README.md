# NoesisNoema ğŸ§ âœ¨

[![GitHub release](https://img.shields.io/github/v/release/raskolnikoff/NoesisNoema)](https://github.com/raskolnikoff/NoesisNoema/releases)
[![Platform](https://img.shields.io/badge/platform-macOS%20%7C%20iOS-blue)](#)
[![Swift](https://img.shields.io/badge/swift-5.9-orange)](#)

Private, offline, multiâ€‘RAGpack LLM RAG for macOS and iOS.
Empower your own AGI â€” no cloud, no SaaS, just your device and your knowledge. ğŸš€

[![YouTube Demo](https://img.youtube.com/vi/VzCrfXZyfss/0.jpg)](https://youtube.com/shorts/VzCrfXZyfss?feature=share)

<img src="docs/assets/rag-main.png" alt="Main UI" height="400" />
<img src="docs/assets/noesisnoema_ios.png" alt="iOS UI" height="400" />

---

## Whatâ€™s New (Aug 2025) ğŸ”¥

The onâ€‘device experience leveled up across macOS and iOS:

- iOS Universal App (WIP â†’ usable today)
  - Fresh iOS screenshot available in `docs/assets/noesisnoema_ios.png` (see above)
  - Alwaysâ€‘visible History, QADetail overlays on top (swipeâ€‘down or âœ–ï¸ to close)
  - Multiline input with placeholder, larger tap targets, equalâ€‘width action buttons
  - Global loading lock to prevent duplicate queries; answer only appended once
  - Keyboard UX: tap outside or scroll to dismiss
  - Startup splash overlay: temporary "Noesis Noema" title on launch
- Output hygiene & stability
  - Streaming filter removes `<think>â€¦</think>` and control tokens; stop at `<|im_end|>`
  - Final normalization unifies model differences for clean, copyâ€‘ready answers
  - Runtime guard detects broken llama.framework loads; lightweight SystemLog
- RAGpack import is stricter and safer
  - Validates presence of `chunks.json` and `embeddings.csv` and enforces count match
  - Deâ€‘duplicates identical chunks across multiple RAGpacks

### Added
- Experimental support for GPTâ€‘OSSâ€‘20B (`gpt-oss-20b-Q4_K_S`, released 2025â€‘08â€‘08). Place the model at `Resources/Models/gpt-oss-20b-Q4_K_S.gguf` and select it in the LLM picker (macOS/iOS).
- LLM Presets (Pocket, Balanced, Pro Max) with deviceâ€‘tuned defaults (threads, GPU layers, batch size, context length, decoding params).
- Liquid Glass design across macOS and iOS with accessibilityâ€‘aware fallbacks (respects Reduce Transparency; solid fallback).

macOS keeps its â€œworkstation feelâ€; iOS now brings the same private RAG, in your pocket. ğŸ“±ğŸ’»

---

## Features âœ¨

- Multiâ€‘RAGpack search and synthesis
- Transversal retrieval across packs (e.g., Kant Ã— Spinoza)
- Fast local inference via llama.cpp + GGUF models
- Private by design: fully offline; no analytics; minimal, local SystemLog (no PII)
- Modern UX
  - Twoâ€‘pane macOS UI
  - iOS: History always visible; QADetail overlays; copyâ€‘able answers; smooth keyboard handling
  - Multiline question input; equalâ€‘width Ask / Choose RAGpack buttons
- Clean answers, consistently
  - `<think>â€¦</think>` is filtered on the fly; control tokens removed; stop tokens respected
- Thin, futureâ€‘proof core
  - llama.cpp through prebuilt xcframeworks (macOS/iOS) with a thin Swift shim
  - Runtime guard + system info log for quick diagnosis

---

## Privacy & Diagnostics ğŸ”’

- 100% offline by default. No network calls for inference or retrieval.
- No analytics SDKs. No telemetry is sent.
- SystemLog is localâ€‘only and minimal (device/OS, model name, params, pack hits, latency, failure reasons). You can optâ€‘in to share diagnostics.

---

## Requirements âœ…

- macOS 13+ (Apple Silicon recommended) or iOS 17+ (A15/Apple Silicon recommended)
- Prebuilt llama xcframeworks (included in this repo):
  - `llama_macos.xcframework`, `llama_ios.xcframework`
- Models in GGUF format
  - Default expected name: `Jan-v1-4B-Q4_K_M.gguf`

> Note (iOS): By default we run CPU fallback for broad device compatibility; real devices are recommended over the simulator for performance.

---

## Quick Start ğŸš€

### macOS (App)
1. Open the project in Xcode.
2. Select the `NoesisNoema` scheme and press Run.
3. Import your RAGpack(s) and start asking questions.

### iOS (App)
1. Select the `NoesisNoemaMobile` scheme.
2. Run on a real device (recommended).
3. Import RAGpack(s) from Files and Ask.
  - History stays visible; QADetail appears as an overlay (swipe down or âœ–ï¸ to close).
  - Return adds a newline in the input; only the Ask button starts inference.

### CLI harness (LlamaBridgeTest) ğŸ§ª
A tiny runner to verify local inference.
- Build the `LlamaBridgeTest` scheme and run with `-p "your prompt"`.
- Uses the same output cleaning to remove `<think>â€¦</think>`.

---

## Using RAGpacks ğŸ“¦

RAGpack is a `.zip` with at least:

- `chunks.json` â€” ordered list of text chunks
- `embeddings.csv` â€” embedding vectors aligned by row
- `metadata.json` â€” optional, bag of properties

Importer safeguards:
- Validates presence of `chunks.json` and `embeddings.csv` and enforces 1:1 count
- Deâ€‘duplicates identical chunk+embedding pairs across packs
- Merges new, unique chunks into the inâ€‘memory vector store

> Tip: Generate RAGpacks with the companion pipeline:
> [noesisnoema-pipeline](https://github.com/raskolnikoff/noesisnoema-pipeline)

---

## Model & Inference ğŸ§©

- NoesisNoema links llama.cpp via prebuilt xcframeworks. You shouldnâ€™t manually embed `llama.framework`; link the xcframework and let Xcode process it.
- Model lookup order (CLI/app): CWD â†’ executable dir â†’ app bundle â†’ `Resources/Models/` â†’ `NoesisNoema/Resources/Models/` â†’ `~/Downloads/`
- Output pipeline:
  - Jan/Qwenâ€‘style prompt where applicable
  - Streamingâ€‘time `<think>` filtering and `<|im_end|>` earlyâ€‘stop
  - Final normalization to erase residual control tokens and selfâ€‘labels

### Deviceâ€‘optimal presets âš™ï¸

- A17/Mâ€‘series: `n_threads = 6â€“8`, `n_gpu_layers = 999`
- A15â€“A16: `n_threads = 4â€“6`, `n_gpu_layers = 40â€“80`
- Generation length: `max_tokens` 128â€“256 (short answers), 400â€“600 (summaries)
- Temperature: 0.2â€“0.4, Topâ€‘K: 40â€“80 for stability

> These are sensible defaults; you can tune per device/pack.

---

## UX Details that Matter ğŸ’…

- iOS
  - Interface preview: see screenshot [noesisnoema_ios.png](docs/assets/noesisnoema_ios.png)
  - Multiline input with placeholder; Return adds a newline (does not send)
  - Only the Ask button can start inference (no accidental double sends)
  - During generation: global overlay lock; all inputs disabled (no duplicate queries)
  - Tap outside or scroll History to dismiss the keyboard
  - QADetail overlays History; close with swipeâ€‘down or âœ–ï¸; answers are textâ€‘selectable and copyable
  - Scroll indicators visible in answers to clarify vertical scroll
- macOS
  - Twoâ€‘pane layout with History and Detail; same output cleaning; quick import

---

## Engineering Policy & Vendor Guardrails ğŸ›¡ï¸

- Vendor code (llama.cpp) is not modified. xcframeworks are prebuilt and checked in.
- Thin shim only: adapt upstream C API in `LibLlama.swift` / `LlamaState.swift`. Other files must not call `llama_*` directly.
- Runtime check: verify `llama.framework` load + symbol presence on startup and log `llama_print_system_info()`.
- If upstream bumps break builds, fix the shim layer and add a unit test before merging.

---

## QA Checklist (releaseâ€‘ready) âœ…

- Accuracy: run same question Ã—3; verify gist stability at low temperature (0.2â€“0.4)
- Latency: measure p50/p90 for short/long prompts and multiâ€‘pack queries; split warm vs warm+1
- Memory/Thermals: 10â€‘question loop; consider thread scaling when throttled
- Failure modes: empty/huge/broken packs; missing model path; userâ€‘facing messages
- Output hygiene: ensure `<think>`/control tokens are absent; newlines preserved
- History durability: ~100 items; startup time and scroll smoothness
- Battery: 15â€‘minute session; confirm best params per device
- Privacy: verify network off; no analytics; README/UI clearly state offline

---

## Troubleshooting ğŸ› ï¸

- `dyld: Library not loaded: @rpath/llama.framework`
  - Clean build folder and DerivedData
  - Link the xcframework only (no manual embed)
  - Ensure Runpath Search Paths include `@executable_path`, `@loader_path`, `@rpath`
- Multiple commands produce `llama.framework`
  - Remove manual â€œEmbed Frameworks/Copy Filesâ€ for the framework; rely on the xcframework
- Model not found
  - Place the model in one of the searched locations or pass an absolute path (CLI)
- iOS keyboard wonâ€™t hide
  - Tap outside the input or scroll History to dismiss
- Output includes control tags or `<think>`
  - Ensure youâ€™re on the latest build; the streaming filter + final normalizer should keep answers clean

---

## Known Issues & FAQ â“

- iOS Simulator is slower and may not reflect real thermals. Prefer running on device.
- Very large RAGpacks can increase memory usage. Prefer chunking and MMR reâ€‘ranking.
- If you still see `<think>` in answers, capture logs and open an issue (modelâ€‘specific templates can slip through).
- Where is `scripts/build_xcframework.sh`?
  - Not included yet. Prebuilt `llama_*.xcframework` are provided in this repo. If you need to rebuild, use upstream llama.cpp build instructions and replace the frameworks under `Frameworks/`.

---

## Roadmap ğŸ—ºï¸

- iOS universal polishing (iPad layouts, sharing/export)
- Enhanced right pane: chunk/source/document previews
- Deviceâ€‘optimal presets and power/thermal controls
- Cloudless peerâ€‘toâ€‘peer sync
- Plugin/API extensibility
- CI for App targets

---

## Ecosystem & Related Projects ğŸŒ

- [RAGfish](https://github.com/raskolnikoff/RAGfish): Core RAGpack specification and toolkit ğŸ“š
- [noesisnoema-pipeline](https://github.com/raskolnikoff/noesisnoema-pipeline): Generate your own RAGpacks from PDF/text ğŸ’¡

---

## Contributing ğŸ¤—

We welcome Designers, Swift/AI/UX developers, and documentation writers.
Open an issue or PR, or join our discussions. See also [RAGfish](https://github.com/raskolnikoff/RAGfish) for the pack spec.

PR Checklist (policy):
- [ ] llama.cpp vendor frameworks unchanged
- [ ] Changes limited to `LibLlama.swift` / `LlamaState.swift` for core llama integration
- [ ] Smoke/Golden/RAG tests passed locally

---

## From the Maintainers ğŸ’¬

This project is not just code â€” itâ€™s our exploration of private AGI, blending philosophy and engineering.  
Each commit is a step toward tools that respect autonomy, curiosity, and the joy of building.  
Stay curious, and contribute if it resonates with you.

ğŸŒŸ
> Your knowledge. Your device. Your rules.

---

## ParamBandit: Thompson Sampling ã§ã‚¯ã‚¨ãƒªã”ã¨ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠ ğŸ”¬

- What: å–å¾—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtop_k, mmr_lambda, min_scoreï¼‰ã‚’ã‚¯ã‚¨ãƒªã‚¯ãƒ©ã‚¹ã‚¿å˜ä½ã§å‹•çš„ã«é¸æŠã™ã‚‹è»½é‡ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã€‚
- Why: æœ€å°é™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã§ç´ æ—©ãé–¢é€£æ€§ã‚’æ”¹å–„ã—ã€å­¦ç¿’ã—ã¦ã„ã‚‹æ‰‹è§¦ã‚Šã‚’æä¾›ã€‚
- Where: ç”Ÿæˆå™¨ã®å‰ã€Retrieval ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç›´å‰ã€‚
- How:
  - å„ã‚¢ãƒ¼ãƒ ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é›†åˆï¼‰ã« Beta(Î±,Î²) ã‚’ä¿æŒã—ã€Thompson Sampling ã§é¸æŠã€‚
  - RewardBus ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆğŸ‘/ğŸ‘ï¼‰ã§ Î±/Î² ã‚’æ›´æ–°ã€‚
  - æ—¢å®šã®ã‚¢ãƒ¼ãƒ ä¾‹: k4/l0.7/s0.20, k5/l0.9/s0.10, k6/l0.7/s0.15, k8/l0.5/s0.15ã€‚

ä½¿ç”¨ä¾‹ï¼ˆçµ±åˆã®è€ƒãˆæ–¹ï¼‰
- æ—¢å­˜ã® LocalRetriever åˆ©ç”¨ãƒã‚¤ãƒ³ãƒˆã®ç›´å‰ã§ ParamBandit ã‚’å‘¼ã³å‡ºã—ã€è¿”å´ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ retrieve ã‚’å®Ÿè¡Œã€‚
- UI å´ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆğŸ‘/ğŸ‘ï¼‰æ™‚ã« RewardBus.shared.publish(qaId:verdict:tags:) ã‚’ç™ºç«ã€‚

ç°¡æ˜“ãƒ•ãƒ­ãƒ¼:
1) let qa = UUID()
2) let choice = ParamBandit.default.chooseParams(for: query, qaId: qa)
3) let ps = choice.arm.params // topK, mmrLambda, minScore
4) let chunks = LocalRetriever(store: .shared).retrieve(query: query, k: ps.topK, lambda: ps.mmrLambda)
5) minScore ã§é¡ä¼¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆBanditRetriever å‚ç…§ï¼‰
6) ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡æ™‚ã« RewardBus.shared.publish(qaId: qa, verdict: .up/.down, tags: â€¦)

ãƒ†ã‚¹ãƒˆã¨DoD
- å˜ä½“: åˆæœŸÎ±=1,Î²=1ã€ğŸ‘ã§Î±++ / ğŸ‘ã§Î²++ ã‚’æ¤œè¨¼ï¼ˆTestRunner ã«è¿½åŠ ã€CLIãƒ“ãƒ«ãƒ‰ã§ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚
- çµ±åˆ: åˆæˆå ±é…¬ã§å„ªè‰¯ã‚¢ãƒ¼ãƒ ã¸é¸å¥½ãŒåæŸã™ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆåŒä¸Šï¼‰ã€‚
- DoD: ParamBandit ã‚’ç‹¬ç«‹ã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦è¿½åŠ ã€RewardBus é€£æºã€æ—¢å®šã‚¢ãƒ¼ãƒ å®šç¾©ã€è»½é‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆæœ¬ç¯€ï¼‰ã‚’æ•´å‚™ã€‚
