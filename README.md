# NoesisNoema ğŸ§ âœ¨

[![GitHub release](https://img.shields.io/github/v/release/raskolnikoff/NoesisNoema)](https://github.com/raskolnikoff/NoesisNoema/releases)
[![Platform](https://img.shields.io/badge/platform-macOS%20%7C%20iOS-blue)](#)
[![Swift](https://img.shields.io/badge/swift-5.9-orange)](#)

Private, offline, multiâ€‘RAGpack LLM RAG for macOS and iOS.
Empower your own AGI â€” no cloud, no SaaS, just your device and your knowledge. ğŸš€

[![YouTube Demo](https://img.youtube.com/vi/VzCrfXZyfss/0.jpg)](https://youtube.com/shorts/VzCrfXZyfss?feature=share)

<img src="docs/assets/rag-main.png" alt="Main UI" height="400" />
<img src="docs/assets/noesisnoema_ios.png" alt="iOS UI" height="400" />

---

## Whatâ€™s New (Aug 2025) ğŸ”¥

The onâ€‘device experience leveled up across macOS and iOS:

- iOS Universal App (iPhone/iPad; shipped)
  - Fresh iOS screenshot available in `docs/assets/noesisnoema_ios.png` (see above)
  - Alwaysâ€‘visible History, QADetail overlays on top (swipeâ€‘down or âœ–ï¸ to close)
  - Multiline input with placeholder, larger tap targets, equalâ€‘width action buttons
  - Global loading lock to prevent duplicate queries; answer only appended once
  - Keyboard UX: tap outside or scroll to dismiss
  - Startup splash overlay: temporary "Noesis Noema" title on launch

- Deep Search retrieval pipeline
  - Two-stage retrieval: LocalRetriever + QueryIterator with MMR reâ€‘ranking
  - Works across multiple RAGpacks; better relevance and source diversity
  - Tuned defaults per device; fast even on iPhone
- Feedback loop (localâ€‘only)
  - Thumbs up/down captured via RewardBus
  - ParamBandit tunes retrieval params per session (topK, mmrLambda, minScore)
  - 100% offline; no telemetry
- Output hygiene & stability
  - Streaming filter removes `<think>â€¦</think>` and control tokens; stop at `<|im_end|>`
  - Final normalization unifies model differences for clean, copyâ€‘ready answers
  - Runtime guard detects broken llama.framework loads; lightweight SystemLog
- RAGpack import is stricter and safer
  - Validates presence of `chunks.json` and `embeddings.csv` and enforces count match
  - Deâ€‘duplicates identical chunks across multiple RAGpacks

### Added
- Experimental support for GPTâ€‘OSSâ€‘20B (`gpt-oss-20b-Q4_K_S`, released 2025â€‘08â€‘08). Place the model at `Resources/Models/gpt-oss-20b-Q4_K_S.gguf` and select it in the LLM picker (macOS/iOS).
- LLM Presets (Pocket, Balanced, Pro Max) with deviceâ€‘tuned defaults (threads, GPU layers, batch size, context length, decoding params).
- Liquid Glass design across macOS and iOS with accessibilityâ€‘aware fallbacks (respects Reduce Transparency; solid fallback).

macOS keeps its â€œworkstation feelâ€; iOS now brings the same private RAG, in your pocket. ğŸ“±ğŸ’»

---

## Features âœ¨

- Multiâ€‘RAGpack search and synthesis
- Transversal retrieval across packs (e.g., Kant Ã— Spinoza)
- Deep Search (query iteration + MMR reâ€‘ranking) with crossâ€‘pack support
- Fast local inference via llama.cpp + GGUF models
- Private by design: fully offline; no analytics; minimal, local SystemLog (no PII)
- Feedback & learning: thumbs up/down feeds ParamBandit to autoâ€‘tune retrieval (sessionâ€‘scoped, offline)
- Modern UX
  - Twoâ€‘pane macOS UI
  - iOS: History always visible; QADetail overlays; copyâ€‘able answers; smooth keyboard handling
  - Multiline question input; equalâ€‘width Ask / Choose RAGpack buttons
- Clean answers, consistently
  - `<think>â€¦</think>` is filtered on the fly; control tokens removed; stop tokens respected
- Thin, futureâ€‘proof core
  - llama.cpp through prebuilt xcframeworks (macOS/iOS) with a thin Swift shim
  - Runtime guard + system info log for quick diagnosis

---

## Privacy & Diagnostics ğŸ”’

- 100% offline by default. No network calls for inference or retrieval.
- No analytics SDKs. No telemetry is sent.
- SystemLog is localâ€‘only and minimal (device/OS, model name, params, pack hits, latency, failure reasons). You can optâ€‘in to share diagnostics.

---

## Requirements âœ…

- macOS 13+ (Apple Silicon recommended) or iOS 17+ (A15/Apple Silicon recommended)
- Prebuilt llama xcframeworks (included in this repo):
  - `llama_macos.xcframework`, `llama_ios.xcframework`
- Models in GGUF format
  - Default expected name: `Jan-v1-4B-Q4_K_M.gguf`

> Note (iOS): By default we run CPU fallback for broad device compatibility; real devices are recommended over the simulator for performance.

---

## Quick Start ğŸš€

### macOS (App)
1. Open the project in Xcode.
2. Select the `NoesisNoema` scheme and press Run.
3. Import your RAGpack(s) and start asking questions.

### iOS (App)
1. Select the `NoesisNoemaMobile` scheme.
2. Run on a real device (recommended).
3. Import RAGpack(s) from Files and Ask.
  - History stays visible; QADetail appears as an overlay (swipe down or âœ–ï¸ to close).
  - Return adds a newline in the input; only the Ask button starts inference.

### CLI harness (LlamaBridgeTest) ğŸ§ª
A tiny runner to verify local inference.
- Build the `LlamaBridgeTest` scheme and run with `-p "your prompt"`.
- Uses the same output cleaning to remove `<think>â€¦</think>`.

---

## Using RAGpacks ğŸ“¦

RAGpack is a `.zip` with at least:

- `chunks.json` â€” ordered list of text chunks
- `embeddings.csv` â€” embedding vectors aligned by row
- `metadata.json` â€” optional, bag of properties

Importer safeguards:
- Validates presence of `chunks.json` and `embeddings.csv` and enforces 1:1 count
- Deâ€‘duplicates identical chunk+embedding pairs across packs
- Merges new, unique chunks into the inâ€‘memory vector store

> Tip: Generate RAGpacks with the companion pipeline:
> [noesisnoema-pipeline](https://github.com/raskolnikoff/noesisnoema-pipeline)

---

## Model & Inference ğŸ§©

- NoesisNoema links llama.cpp via prebuilt xcframeworks. You shouldnâ€™t manually embed `llama.framework`; link the xcframework and let Xcode process it.
- Model lookup order (CLI/app): CWD â†’ executable dir â†’ app bundle â†’ `Resources/Models/` â†’ `NoesisNoema/Resources/Models/` â†’ `~/Downloads/`
- Output pipeline:
  - Jan/Qwenâ€‘style prompt where applicable
  - Streamingâ€‘time `<think>` filtering and `<|im_end|>` earlyâ€‘stop
  - Final normalization to erase residual control tokens and selfâ€‘labels

### Deviceâ€‘optimal presets âš™ï¸

- A17/Mâ€‘series: `n_threads = 6â€“8`, `n_gpu_layers = 999`
- A15â€“A16: `n_threads = 4â€“6`, `n_gpu_layers = 40â€“80`
- Generation length: `max_tokens` 128â€“256 (short answers), 400â€“600 (summaries)
- Temperature: 0.2â€“0.4, Topâ€‘K: 40â€“80 for stability

> These are sensible defaults; you can tune per device/pack.

---

## UX Details that Matter ğŸ’…

- iOS
  - Interface preview: see screenshot [noesisnoema_ios.png](docs/assets/noesisnoema_ios.png)
  - Multiline input with placeholder; Return adds a newline (does not send)
  - Only the Ask button can start inference (no accidental double sends)
  - During generation: global overlay lock; all inputs disabled (no duplicate queries)
  - Tap outside or scroll History to dismiss the keyboard
  - QADetail overlays History; close with swipeâ€‘down or âœ–ï¸; answers are textâ€‘selectable and copyable
  - Scroll indicators visible in answers to clarify vertical scroll
- macOS
  - Twoâ€‘pane layout with History and Detail; same output cleaning; quick import

---

## Engineering Policy & Vendor Guardrails ğŸ›¡ï¸

- Vendor code (llama.cpp) is not modified. xcframeworks are prebuilt and checked in.
- Thin shim only: adapt upstream C API in `LibLlama.swift` / `LlamaState.swift`. Other files must not call `llama_*` directly.
- Runtime check: verify `llama.framework` load + symbol presence on startup and log `llama_print_system_info()`.
- If upstream bumps break builds, fix the shim layer and add a unit test before merging.

---

## QA Checklist (releaseâ€‘ready) âœ…

- Accuracy: run same question Ã—3; verify gist stability at low temperature (0.2â€“0.4)
- Latency: measure p50/p90 for short/long prompts and multiâ€‘pack queries; split warm vs warm+1
- Memory/Thermals: 10â€‘question loop; consider thread scaling when throttled
- Failure modes: empty/huge/broken packs; missing model path; userâ€‘facing messages
- Output hygiene: ensure `<think>`/control tokens are absent; newlines preserved
- History durability: ~100 items; startup time and scroll smoothness
- Battery: 15â€‘minute session; confirm best params per device
- Privacy: verify network off; no analytics; README/UI clearly state offline

---

## Troubleshooting ğŸ› ï¸

- `dyld: Library not loaded: @rpath/llama.framework`
  - Clean build folder and DerivedData
  - Link the xcframework only (no manual embed)
  - Ensure Runpath Search Paths include `@executable_path`, `@loader_path`, `@rpath`
- Multiple commands produce `llama.framework`
  - Remove manual â€œEmbed Frameworks/Copy Filesâ€ for the framework; rely on the xcframework
- Model not found
  - Place the model in one of the searched locations or pass an absolute path (CLI)
- iOS keyboard wonâ€™t hide
  - Tap outside the input or scroll History to dismiss
- Output includes control tags or `<think>`
  - Ensure youâ€™re on the latest build; the streaming filter + final normalizer should keep answers clean

---

## Known Issues & FAQ â“

- iOS Simulator is slower and may not reflect real thermals. Prefer running on device.
- Very large RAGpacks can increase memory usage. Prefer chunking and MMR reâ€‘ranking.
- If you still see `<think>` in answers, capture logs and open an issue (modelâ€‘specific templates can slip through).
- Where is `scripts/build_xcframework.sh`?
  - Not included yet. Prebuilt `llama_*.xcframework` are provided in this repo. If you need to rebuild, use upstream llama.cpp build instructions and replace the frameworks under `Frameworks/`.

---

## Roadmap ğŸ—ºï¸

- iOS universal polishing (iPad layouts, sharing/export)
- Enhanced right pane: chunk/source/document previews
- Power/thermal controls (deviceâ€‘aware throttling)
- Cloudless peerâ€‘toâ€‘peer sync
- Plugin/API extensibility
- CI for App targets

---

## Ecosystem & Related Projects ğŸŒ

- [RAGfish](https://github.com/raskolnikoff/RAGfish): Core RAGpack specification and toolkit ğŸ“š
- [noesisnoema-pipeline](https://github.com/raskolnikoff/noesisnoema-pipeline): Generate your own RAGpacks from PDF/text ğŸ’¡

---

## Contributing ğŸ¤—

We welcome Designers, Swift/AI/UX developers, and documentation writers.
Open an issue or PR, or join our discussions. See also [RAGfish](https://github.com/raskolnikoff/RAGfish) for the pack spec.

PR Checklist (policy):
- [ ] llama.cpp vendor frameworks unchanged
- [ ] Changes limited to `LibLlama.swift` / `LlamaState.swift` for core llama integration
- [ ] Smoke/Golden/RAG tests passed locally

---

## From the Maintainers ğŸ’¬

This project is not just code â€” itâ€™s our exploration of private AGI, blending philosophy and engineering.  
Each commit is a step toward tools that respect autonomy, curiosity, and the joy of building.  
Stay curious, and contribute if it resonates with you.

ğŸŒŸ
> Your knowledge. Your device. Your rules.

---
## ParamBandit: Thompson Sampling to Select Optimal Parameters per Query ğŸ”¬

- What: A lightweight bandit that dynamically selects retrieval parameters (top_k, mmr_lambda, min_score) per query cluster.
- Why: Quickly improves relevance with minimal feedback and provides the feeling of a system that is learning.
- Where: Just before the generator, immediately before the retrieval pipeline.
- How:
  - Maintains Beta(Î±,Î²) distributions for each arm (parameter set) and selects using Thompson Sampling.
  - Updates Î±/Î² based on feedback events (ğŸ‘/ğŸ‘) from the RewardBus.
  - Example default arms: k4/l0.7/s0.20, k5/l0.9/s0.10, k6/l0.7/s0.15, k8/l0.5/s0.15.

Usage example (integration concept)
- Call ParamBandit just before existing LocalRetriever usage points, and perform retrieval with the returned parameters.
- On the UI side, trigger RewardBus.shared.publish(qaId:verdict:tags:) upon user feedback (ğŸ‘/ğŸ‘).

Simplified flow:
1) let qa = UUID()
2) let choice = ParamBandit.default.chooseParams(for: query, qaId: qa)
3) let ps = choice.arm.params // topK, mmrLambda, minScore
4) let chunks = LocalRetriever(store: .shared).retrieve(query: query, k: ps.topK, lambda: ps.mmrLambda)
5) Filter by minScore for similarity (see BanditRetriever)
6) On user evaluation, call RewardBus.shared.publish(qaId: qa, verdict: .up/.down, tags: â€¦)

Tests and Definition of Done (DoD)
- Unit: Verify initial Î±=1, Î²=1, and that ğŸ‘ increments Î± and ğŸ‘ increments Î² (add to TestRunner, skip in CLI build).
- Integration: Confirm preference converges to the best arm with composite rewards (same as above).
- DoD: Add ParamBandit as an independent service, integrate with RewardBus, define default arms, and provide lightweight documentation (this section).
